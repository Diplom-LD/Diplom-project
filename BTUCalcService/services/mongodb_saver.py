import hashlib
import json
from datetime import datetime
from pymongo.collection import Collection
from pymongo.database import Database
from pymongo.errors import BulkWriteError
from pymongo import ReplaceOne


def calculate_overall_hash(products: list[dict]) -> str:
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Ö—ç—à –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤."""
    data_to_hash = []
    for product in products:
        clean_product = {k: v for k, v in product.items() if k not in {"_id", "hash", "updated_at"}}
        data_to_hash.append(clean_product)

    data_string = json.dumps(data_to_hash, sort_keys=True, ensure_ascii=False)
    return hashlib.sha256(data_string.encode()).hexdigest()


class MongoDBParserSaver:
    def __init__(self, db: Database):
        self.db = db

    def save_products(self, parser_name: str, products: list[dict]) -> bool:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–¥—É–∫—Ç—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö."""
        collection: Collection = self.db[f"{parser_name}_products"]
        all_products_collection: Collection = self.db["all_products"]

        if not products:
            print(f"[{parser_name}] ‚ùå –ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤. –ü—Ä–æ–ø—É—Å–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.")
            return False

        overall_hash = calculate_overall_hash(products)
        print(f"[{parser_name}] üîê –ù–æ–≤—ã–π —Ö—ç—à: {overall_hash}")

        metadata = collection.find_one({"_id": "metadata"})
        current_db_hash = metadata["hash"] if metadata else None
        print(f"[{parser_name}] üì¶ –¢–µ–∫—É—â–∏–π —Ö—ç—à –≤ –ë–î: {current_db_hash}")

        if current_db_hash == overall_hash:
            print(f"[{parser_name}] ‚ÑπÔ∏è –•—ç—à –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
            return False

        print(f"[{parser_name}] üîÅ –•—ç—à –∏–∑–º–µ–Ω–∏–ª—Å—è. –ù–∞—á–∏–Ω–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ...")

        collection.delete_many({"_id": {"$ne": "metadata"}})

        collection.update_one(
            {"_id": "metadata"},
            {"$set": {"hash": overall_hash, "updated_at": datetime.utcnow()}},
            upsert=True
        )

        bulk_operations = []
        bulk_operations_all = []

        updated_count = 0
        inserted_count = 0

        for product in products:
            if "url" not in product or not product["url"]:
                print(f"[{parser_name}] ‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω —Ç–æ–≤–∞—Ä –±–µ–∑ 'url': {product}")
                continue

            product["_id"] = product["url"]
            product["updated_at"] = datetime.utcnow()

            if collection.find_one({"_id": product["_id"]}):
                updated_count += 1
            else:
                inserted_count += 1

            bulk_operations.append(
                ReplaceOne({"_id": product["_id"]}, product, upsert=True)
            )

            product_copy = product.copy()
            product_copy["_id"] = f"{parser_name}_{product['url']}"
            product_copy["source"] = parser_name
            bulk_operations_all.append(
                ReplaceOne({"_id": product_copy["_id"]}, product_copy, upsert=True)
            )

        try:
            if bulk_operations:
                result = collection.bulk_write(bulk_operations)
                print(f"[{parser_name}] ‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ {updated_count}, –¥–æ–±–∞–≤–ª–µ–Ω–æ {inserted_count} —Ç–æ–≤–∞—Ä–æ–≤.")

            if bulk_operations_all:
                # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Ç–æ–≤–∞—Ä—ã —ç—Ç–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞ –≤ –æ–±—â–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏
                all_products_collection.delete_many({"source": parser_name})
                result_all = all_products_collection.bulk_write(bulk_operations_all)
                print(f"[all_products] ‚úÖ –ó–∞–ø–∏—Å–∞–Ω–æ {len(bulk_operations_all)} —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ {parser_name}.")
                print(f"[all_products] MongoDB —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {result_all.bulk_api_result}")

        except BulkWriteError as e:
            print(f"[{parser_name}] ‚ùå –û—à–∏–±–∫–∞ –º–∞—Å—Å–æ–≤–æ–π –∑–∞–ø–∏—Å–∏: {e.details}")
            return False

        return True
